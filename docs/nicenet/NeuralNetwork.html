<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>nicenet.NeuralNetwork API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>nicenet.NeuralNetwork</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#!/usr/bin/python3

import numpy as np
from matplotlib import pyplot as plt
from .Layer import Layer
from .Dataset import Dataset
import json
import time
from .LossFunctions import LossFunctions
import typing
from .Types import T_Feature_Array, T_Target_Array, T_Output_Array, T_Data_Sample, T_Dataset
from tqdm import tqdm
from . import Utils

np.set_printoptions(precision=20)


class NeuralNetwork:
    # Main NeuralNetwork class
    def __init__(self, I, O, cost=&#34;mse&#34;):
        &#34;&#34;&#34;
        Creates a Feed Forward Neural Network.

        Parameters
        ----------
        I : int
            Number of inputs to the network

        O : int
            Number of outputs from the network

        [cost]: string
            The cost/loss function used by the neural network.
            Default value is &#39;mse&#39; which stands for Mean Squared Error.

            Available options:
                mse =&gt; Mean Squared Error
                ce =&gt; Cross Entropy

        Returns
        -------
        Doesn&#39;t return anything
        &#34;&#34;&#34;

        # print(&#34;Construct&#34;)
        self.Network: typing.List[Layer] = list()
        self.I = I
        self.O = O
        self.cost = cost
        self.loss_computer = LossFunctions(cost)
        self.total_layers = 0
        self.learningRate = 0.01
        self.isLoadedModel = False
        self.model_compiled = False
        self.prediction_evaulator = Utils.judge_prediction

    def setLearningRate(self, lr):
        &#34;&#34;&#34;
        Modifies the learning rate of the network.

        Parameters
        ----------
        lr : float
            New learning rate

        Returns
        -------
        Doesn&#39;t return anything
        &#34;&#34;&#34;
        self.learningRate = lr

    def addLayer(self, num_nodes, activation_function=&#34;sigmoid&#34;):
        &#34;&#34;&#34;
        Adds a layer to the network.

        Parameters
        ----------
        num_nodes : int
            Number of nodes in the hidden layer

        [activation_function] :str
            It is an optional parameter.
            Specifies the activation function of the layer.
            Default value is sigmoid.

            Available options:
                sigmoid,
                tanh,
                linear,
                identity,
                softmax

        Returns
        -------
        Doesn&#39;t return anything
        &#34;&#34;&#34;
        # A layer can be thought of as a matrix
        # No. of row = no. of nodes
        # No. of columns = No. of weights = No. of inputs + 1 (bias)
        if self.total_layers == 0:
            inputs = self.I
        else:
            last_layer = self.Network[-1]
            inputs = last_layer.num_nodes
        layer = Layer(num_nodes, inputs, activation_function, self.cost)
        self.Network.append(layer)
        self.total_layers += 1

    def compile(self, activation_function=&#34;sigmoid&#34;):
        &#34;&#34;&#34;
        Basically, it just adds the output layer to the network.

        Parameters
        ----------
        [activation_function] :str
            It is an optional parameter.
            Specifies the activation function of the layer.
            Default value is sigmoid.

        Returns
        -------
        Doesn&#39;t return anything
        &#34;&#34;&#34;
        if self.model_compiled:
            print(&#34;[!!] Model is already compiled!&#34;)
            print(&#34;[!!] You cannot add layers anymore&#34;)
            return
        self.isLoadedModel = False
        self.model_compiled = True
        # Adding output layer
        self.addLayer(self.O, activation_function=activation_function)

        # for layer in self.Network :
        #     print(np.mean(layer.weights), np.std(layer.weights))
        #     print(np.mean(layer.biases), np.std(layer.biases))
        #     print()

    def feedforward(self, input_array: T_Feature_Array):
        &#34;&#34;&#34;
        Feeds the given input throughout the network

        Parameters
        ----------
        input_array : T_Feature_Array
            Input to be fed to the network.
            It is columnar vector of size Inputs x 1

        Returns
        -------
        all_outputs : T_Output_Array
            An array of all the outputs produced by each layer.
        &#34;&#34;&#34;
        all_outputs: typing.List[T_Output_Array] = list()
        _i = 1
        for layer in self.Network:
            # print(&#34;Feeding &#34;, input_array.T, &#34;to , layer&#34;, i)
            outputs = layer.feed(input_array)
            all_outputs.append(outputs.T)
            input_array = outputs
            # print(&#34;All outputs: &#34;, all_outputs)
            # print()
            # i += 1
        return all_outputs

    def backpropagate(self, target: T_Target_Array):
        &#34;&#34;&#34;
        Backpropagate the error throughout the network
        This function is called inside the model only.

        Parameters
        ----------
        target : np.array()
            It is the ground truth value corresponding to the input.
            It is columnar vector of size Outputs x 1

        Returns
        -------
        Error : float
            # Returns the Mean Squared Error of the particular output
            Returns the error using the specified loss function.
        &#34;&#34;&#34;
        for i in range(self.total_layers - 1, -1, -1):
            layer = self.Network[i]
            if i == self.total_layers - 1:
                # print(&#34;Output layer: &#34;, layer.output_array, &#34;Target: &#34;, target)
                output_errors = self.loss_computer.get_loss(
                    layer.output_array, target)

                # Evaluate this prediction to compute accuracy
                is_correct_output: bool = self.prediction_evaulator(
                    layer.output_array, target)

                # print(&#34;Error: &#34;, output_errors)
                layer.calculate_gradients(target, &#34;output&#34;)
            else:
                next_layer = self.Network[i + 1]
                layer.calculate_gradients(
                    next_layer.weights, &#34;hidden&#34;, next_layer.deltas
                )
        return output_errors, is_correct_output

    def update_weights(self, input_array: T_Feature_Array):
        &#34;&#34;&#34;
        Update the weights of the network.
        This function is called inside the model only.

        Parameters
        ----------
        input_array : np.array()
            It is the input fed to the network
            It is columnar vector of size Inputs x 1

        Returns
        -------
        Doesn&#39;t return anything
        &#34;&#34;&#34;
        for i in range(self.total_layers - 1, -1, -1):
            layer = self.Network[i]
            if i == 0:
                # if it is the first layer =&gt; inputs = input_array
                layer.update_weights(input_array, self.learningRate)
            else:
                # not the first most =&gt; inputs = previous layer&#39;s output
                inputs = self.Network[i - 1].output_array
                layer.update_weights(inputs, self.learningRate)

    def Train(self, dataset: T_Dataset, size, epochs=100, logging=False, epoch_logging=True, prediction_evaulator=None):
        &#34;&#34;&#34;
        Trains the neural network using the given dataset.

        Parameters
        ----------
        dataset : T_Dataset

        size : int
            Size of the dataset

        [epochs] : int
            An optional parameter.
            Number of epochs to train the network. Default value is 5000

        [logging] : bool
            An optional parameter.
            If its true, all outputs from the network will be logged out onto STDOUT for each epoch.

        [epoch_logging] : bool
            An optional parameter.
            If it is true, Error in each epoch will be logged to STDOUT.

        [prediction_evaulator]: (prediction: T_Output_Array, target: T_Output_Array) -&gt; bool
            An optional parameter.
            Used to evaluate the fed forward output with the actual target.
            Default value is &#39;Utils.judge_prediction&#39; function.

        Returns
        -------
        Doesn&#39;t return anything.
        &#34;&#34;&#34;

        if prediction_evaulator is not None:
            self.prediction_evaulator = prediction_evaulator

        if not self.model_compiled:
            print(&#34;[-] Network is not complete.!&#34;)
            print(&#34;[!!] Please compile the network by adding an output layer&#34;)
            return

        self.all_errors = list()
        self.epochs = epochs

        # Show progress bar if epoch_logging is False
        range_object = range(epochs)
        if not epoch_logging:
            range_object = tqdm(range(epochs), desc=&#34;Training...&#34;)

        for epoch in range_object:
            self.loss = 0
            # One Epoch
            if epoch == 0:
                start = time.time()
            elif epoch == 1:
                end = time.time()
                estimated_time = Utils.get_time_required(start, end, epochs)
                print(estimated_time)

            correct = 0
            for i in range(size):
                data_sample = dataset[i]
                # input_array = data_sample[0]
                # target_array = data_sample[1]
                input_array = data_sample[0]
                target_array = data_sample[1]

                all_outputs = self.feedforward(input_array)
                output_error, is_correct_output = self.backpropagate(
                    target_array)
                self.update_weights(input_array)

                if is_correct_output:
                    correct += 1
                self.loss += output_error
                if logging:
                    print(
                        input_array.transpose(),
                        &#34;\x1b[35m&#34;,
                        all_outputs,
                        &#34;\x1b[0m&#34;,
                        target_array.transpose(),
                        &#34;\x1b[31m&#34;,
                        output_error,
                        &#34;\x1b[0m&#34;,
                    )

            self.loss /= size
            self.all_errors.append(self.loss)
            self.accuracy = (correct*100)/size

            if epoch_logging:
                print(
                    f&#34;Epoch: {epoch + 1} ==&gt; ({self.cost}) Error: {self.loss}, (%) Accuracy: {self.accuracy}&#34;)

            if logging:
                print()

    def predict(self, input_array: T_Feature_Array):
        &#34;&#34;&#34;
        Predicts the output using a given input.

        Parameters
        ----------
        input_array : np.array()
            It is columnar vector of size Inputs x 1
            It is the input fed to the network

        Returns
        -------
        prediction : np.array()
            Predicted value produced by the network.
        &#34;&#34;&#34;
        return self.feedforward(input_array)[-1]

    def epoch_vs_error(self):
        &#34;&#34;&#34;
        Plot error vs epoch graph

        Parameters
        ----------
        Doesn&#39;t accept any parameters

        Returns
        -------
        Doesn&#39;t return anything
        &#34;&#34;&#34;
        if self.isLoadedModel:
            print(&#34;[!!] You cannot look at epoch vs error graph in a loaded model&#34;)
            print(&#34;[!!] You can only look at that while training.!&#34;)
            print(&#34;[!!] Make some modifications to the network to own the model&#34;)
            return
        all_epochs = [i + 1 for i in range(self.epochs)]
        plt.xlabel(&#34;Epoch&#34;)
        plt.ylabel(&#34;Error&#34;)
        plt.title(&#34;Epoch vs Error&#34;)
        plt.plot(all_epochs, self.all_errors)
        plt.show()

    def evaluate(self):
        &#34;&#34;&#34;
        Print the basic information about the network.
        Like accuracy, error ..etc.

        Parameters
        ----------
        Doesn&#39;t accept any parameters

        Returns
        -------
        Doesn&#39;t return anything
        &#34;&#34;&#34;
        print(&#34;\t=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-&#34;)
        print(&#34;\tModel is trained for &#34;, self.epochs, &#34;epochs&#34;)
        print(&#34;\tModel Accuracy: &#34;, self.accuracy, &#34;%&#34;)
        if self.accuracy &lt; 70:
            print(&#34;\t\tModel Doesn&#39;t seem to have fit the data correctly&#34;)
            print(&#34;\t\tTry increasing the epochs&#34;)
        print(&#34;\t=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-&#34;)

    def display(self):
        &#34;&#34;&#34;
        Print the information of each layer of the network.
        It can be used to debug the network!

        Parameters
        ----------
        Doesn&#39;t accept any parameters

        Returns
        -------
        Doesn&#39;t return anything
        &#34;&#34;&#34;
        for i in range(self.total_layers):
            print(&#34;Layer: &#34;, i + 1)
            self.Network[i].display()

    def export_model(self, filename):
        &#34;&#34;&#34;
        Export the model to a json file

        Parameters
        ----------
        filename: str
            File name to export model

        Returns
        -------
        Doesn&#39;t return anything
        &#34;&#34;&#34;
        try:
            fhand = open(filename, &#34;w&#34;)
        except Exception as e:
            print(&#34;[!!] Unable to open file &#34;, filename, &#34;: &#34;, e)
            print(&#34;[!!] Couldn&#39;t export model&#34;)
            return

        model_info = dict()
        model_info[&#34;inputs&#34;] = self.I
        model_info[&#34;outputs&#34;] = self.O
        model_info[&#34;learning_rate&#34;] = self.learningRate
        model_info[&#34;model_compiled&#34;] = self.model_compiled
        model_info[&#34;layers&#34;] = list()
        for layer in self.Network:
            layer_object = dict()
            layer_object[&#34;neurons&#34;] = layer.num_nodes
            layer_object[&#34;inputs&#34;] = layer.inputs
            layer_object[&#34;weights&#34;] = layer.weights.tolist()
            layer_object[&#34;biases&#34;] = layer.biases.tolist()
            layer_object[&#34;activation_function&#34;] = layer.activation_function
            layer_object[&#34;loss_function&#34;] = layer.loss_function
            model_info[&#34;layers&#34;].append(layer_object)
        model_info[&#34;accuracy&#34;] = self.accuracy
        model_info[&#34;loss&#34;] = self.loss
        model_info[&#34;epochs&#34;] = self.epochs

        json_format_string = json.dumps(model_info)
        fhand.write(json_format_string)
        fhand.close()
        print(&#34;[*] Model exported successfully to&#34;, filename)

    @staticmethod
    def load_model(filename):
        &#34;&#34;&#34;
        Load model from an eported (json) model

        Parameters
        ----------
        filename : str
            Exported model (json) file

        Returns
        -------
        brain : NeuralNetwork
            NeuralNetwork object
        &#34;&#34;&#34;
        try:
            fhand = open(filename, &#34;r&#34;)
        except Exception as e:
            print(&#34;[!!] Unable to open file &#34;, filename, &#34;: &#34;, e)
            print(&#34;[!!] Couldn&#39;t load model&#34;)
            return

        model_info = json.load(fhand)
        # print(model_info)

        inputs = model_info[&#34;inputs&#34;]
        outputs = model_info[&#34;outputs&#34;]

        brain = NeuralNetwork(inputs, outputs)
        brain.total_layers = 0
        brain.isLoadedModel = True

        for layer_object in model_info[&#34;layers&#34;]:
            num_nodes = layer_object[&#34;neurons&#34;]
            weights = layer_object[&#34;weights&#34;]
            inputs = layer_object[&#34;inputs&#34;]
            biases = layer_object[&#34;biases&#34;]
            activation_fn = layer_object[&#34;activation_function&#34;]
            loss_function = layer_object[&#34;loss_function&#34;]

            layer = Layer(
                num_nodes, inputs, activation_function=activation_fn, loss_function=loss_function)
            layer.weights = np.array(weights)
            layer.biases = np.array(biases)
            brain.Network.append(layer)
            brain.total_layers += 1

        brain.accuracy = model_info[&#34;accuracy&#34;]
        brain.loss = model_info[&#34;loss&#34;]
        brain.epochs = model_info[&#34;epochs&#34;]
        brain.learningRate = model_info[&#34;learning_rate&#34;]
        brain.model_compiled = model_info[&#34;model_compiled&#34;]
        print(&#34;[*] (&#34;, filename, &#34;) Model Loaded successfully&#34;, sep=&#34;&#34;)

        return brain</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="nicenet.NeuralNetwork.NeuralNetwork"><code class="flex name class">
<span>class <span class="ident">NeuralNetwork</span></span>
<span>(</span><span>I, O, cost='mse')</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a Feed Forward Neural Network.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>I</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of inputs to the network</dd>
<dt><strong><code>O</code></strong> :&ensp;<code>int</code></dt>
<dd>
<p>Number of outputs from the network</p>
<p>The cost/loss function used by the neural network.
Default value is 'mse' which stands for Mean Squared Error.</p>
<p>Available options:
mse =&gt; Mean Squared Error
ce =&gt; Cross Entropy</p>
</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Doesn't return anything</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class NeuralNetwork:
    # Main NeuralNetwork class
    def __init__(self, I, O, cost=&#34;mse&#34;):
        &#34;&#34;&#34;
        Creates a Feed Forward Neural Network.

        Parameters
        ----------
        I : int
            Number of inputs to the network

        O : int
            Number of outputs from the network

        [cost]: string
            The cost/loss function used by the neural network.
            Default value is &#39;mse&#39; which stands for Mean Squared Error.

            Available options:
                mse =&gt; Mean Squared Error
                ce =&gt; Cross Entropy

        Returns
        -------
        Doesn&#39;t return anything
        &#34;&#34;&#34;

        # print(&#34;Construct&#34;)
        self.Network: typing.List[Layer] = list()
        self.I = I
        self.O = O
        self.cost = cost
        self.loss_computer = LossFunctions(cost)
        self.total_layers = 0
        self.learningRate = 0.01
        self.isLoadedModel = False
        self.model_compiled = False
        self.prediction_evaulator = Utils.judge_prediction

    def setLearningRate(self, lr):
        &#34;&#34;&#34;
        Modifies the learning rate of the network.

        Parameters
        ----------
        lr : float
            New learning rate

        Returns
        -------
        Doesn&#39;t return anything
        &#34;&#34;&#34;
        self.learningRate = lr

    def addLayer(self, num_nodes, activation_function=&#34;sigmoid&#34;):
        &#34;&#34;&#34;
        Adds a layer to the network.

        Parameters
        ----------
        num_nodes : int
            Number of nodes in the hidden layer

        [activation_function] :str
            It is an optional parameter.
            Specifies the activation function of the layer.
            Default value is sigmoid.

            Available options:
                sigmoid,
                tanh,
                linear,
                identity,
                softmax

        Returns
        -------
        Doesn&#39;t return anything
        &#34;&#34;&#34;
        # A layer can be thought of as a matrix
        # No. of row = no. of nodes
        # No. of columns = No. of weights = No. of inputs + 1 (bias)
        if self.total_layers == 0:
            inputs = self.I
        else:
            last_layer = self.Network[-1]
            inputs = last_layer.num_nodes
        layer = Layer(num_nodes, inputs, activation_function, self.cost)
        self.Network.append(layer)
        self.total_layers += 1

    def compile(self, activation_function=&#34;sigmoid&#34;):
        &#34;&#34;&#34;
        Basically, it just adds the output layer to the network.

        Parameters
        ----------
        [activation_function] :str
            It is an optional parameter.
            Specifies the activation function of the layer.
            Default value is sigmoid.

        Returns
        -------
        Doesn&#39;t return anything
        &#34;&#34;&#34;
        if self.model_compiled:
            print(&#34;[!!] Model is already compiled!&#34;)
            print(&#34;[!!] You cannot add layers anymore&#34;)
            return
        self.isLoadedModel = False
        self.model_compiled = True
        # Adding output layer
        self.addLayer(self.O, activation_function=activation_function)

        # for layer in self.Network :
        #     print(np.mean(layer.weights), np.std(layer.weights))
        #     print(np.mean(layer.biases), np.std(layer.biases))
        #     print()

    def feedforward(self, input_array: T_Feature_Array):
        &#34;&#34;&#34;
        Feeds the given input throughout the network

        Parameters
        ----------
        input_array : T_Feature_Array
            Input to be fed to the network.
            It is columnar vector of size Inputs x 1

        Returns
        -------
        all_outputs : T_Output_Array
            An array of all the outputs produced by each layer.
        &#34;&#34;&#34;
        all_outputs: typing.List[T_Output_Array] = list()
        _i = 1
        for layer in self.Network:
            # print(&#34;Feeding &#34;, input_array.T, &#34;to , layer&#34;, i)
            outputs = layer.feed(input_array)
            all_outputs.append(outputs.T)
            input_array = outputs
            # print(&#34;All outputs: &#34;, all_outputs)
            # print()
            # i += 1
        return all_outputs

    def backpropagate(self, target: T_Target_Array):
        &#34;&#34;&#34;
        Backpropagate the error throughout the network
        This function is called inside the model only.

        Parameters
        ----------
        target : np.array()
            It is the ground truth value corresponding to the input.
            It is columnar vector of size Outputs x 1

        Returns
        -------
        Error : float
            # Returns the Mean Squared Error of the particular output
            Returns the error using the specified loss function.
        &#34;&#34;&#34;
        for i in range(self.total_layers - 1, -1, -1):
            layer = self.Network[i]
            if i == self.total_layers - 1:
                # print(&#34;Output layer: &#34;, layer.output_array, &#34;Target: &#34;, target)
                output_errors = self.loss_computer.get_loss(
                    layer.output_array, target)

                # Evaluate this prediction to compute accuracy
                is_correct_output: bool = self.prediction_evaulator(
                    layer.output_array, target)

                # print(&#34;Error: &#34;, output_errors)
                layer.calculate_gradients(target, &#34;output&#34;)
            else:
                next_layer = self.Network[i + 1]
                layer.calculate_gradients(
                    next_layer.weights, &#34;hidden&#34;, next_layer.deltas
                )
        return output_errors, is_correct_output

    def update_weights(self, input_array: T_Feature_Array):
        &#34;&#34;&#34;
        Update the weights of the network.
        This function is called inside the model only.

        Parameters
        ----------
        input_array : np.array()
            It is the input fed to the network
            It is columnar vector of size Inputs x 1

        Returns
        -------
        Doesn&#39;t return anything
        &#34;&#34;&#34;
        for i in range(self.total_layers - 1, -1, -1):
            layer = self.Network[i]
            if i == 0:
                # if it is the first layer =&gt; inputs = input_array
                layer.update_weights(input_array, self.learningRate)
            else:
                # not the first most =&gt; inputs = previous layer&#39;s output
                inputs = self.Network[i - 1].output_array
                layer.update_weights(inputs, self.learningRate)

    def Train(self, dataset: T_Dataset, size, epochs=100, logging=False, epoch_logging=True, prediction_evaulator=None):
        &#34;&#34;&#34;
        Trains the neural network using the given dataset.

        Parameters
        ----------
        dataset : T_Dataset

        size : int
            Size of the dataset

        [epochs] : int
            An optional parameter.
            Number of epochs to train the network. Default value is 5000

        [logging] : bool
            An optional parameter.
            If its true, all outputs from the network will be logged out onto STDOUT for each epoch.

        [epoch_logging] : bool
            An optional parameter.
            If it is true, Error in each epoch will be logged to STDOUT.

        [prediction_evaulator]: (prediction: T_Output_Array, target: T_Output_Array) -&gt; bool
            An optional parameter.
            Used to evaluate the fed forward output with the actual target.
            Default value is &#39;Utils.judge_prediction&#39; function.

        Returns
        -------
        Doesn&#39;t return anything.
        &#34;&#34;&#34;

        if prediction_evaulator is not None:
            self.prediction_evaulator = prediction_evaulator

        if not self.model_compiled:
            print(&#34;[-] Network is not complete.!&#34;)
            print(&#34;[!!] Please compile the network by adding an output layer&#34;)
            return

        self.all_errors = list()
        self.epochs = epochs

        # Show progress bar if epoch_logging is False
        range_object = range(epochs)
        if not epoch_logging:
            range_object = tqdm(range(epochs), desc=&#34;Training...&#34;)

        for epoch in range_object:
            self.loss = 0
            # One Epoch
            if epoch == 0:
                start = time.time()
            elif epoch == 1:
                end = time.time()
                estimated_time = Utils.get_time_required(start, end, epochs)
                print(estimated_time)

            correct = 0
            for i in range(size):
                data_sample = dataset[i]
                # input_array = data_sample[0]
                # target_array = data_sample[1]
                input_array = data_sample[0]
                target_array = data_sample[1]

                all_outputs = self.feedforward(input_array)
                output_error, is_correct_output = self.backpropagate(
                    target_array)
                self.update_weights(input_array)

                if is_correct_output:
                    correct += 1
                self.loss += output_error
                if logging:
                    print(
                        input_array.transpose(),
                        &#34;\x1b[35m&#34;,
                        all_outputs,
                        &#34;\x1b[0m&#34;,
                        target_array.transpose(),
                        &#34;\x1b[31m&#34;,
                        output_error,
                        &#34;\x1b[0m&#34;,
                    )

            self.loss /= size
            self.all_errors.append(self.loss)
            self.accuracy = (correct*100)/size

            if epoch_logging:
                print(
                    f&#34;Epoch: {epoch + 1} ==&gt; ({self.cost}) Error: {self.loss}, (%) Accuracy: {self.accuracy}&#34;)

            if logging:
                print()

    def predict(self, input_array: T_Feature_Array):
        &#34;&#34;&#34;
        Predicts the output using a given input.

        Parameters
        ----------
        input_array : np.array()
            It is columnar vector of size Inputs x 1
            It is the input fed to the network

        Returns
        -------
        prediction : np.array()
            Predicted value produced by the network.
        &#34;&#34;&#34;
        return self.feedforward(input_array)[-1]

    def epoch_vs_error(self):
        &#34;&#34;&#34;
        Plot error vs epoch graph

        Parameters
        ----------
        Doesn&#39;t accept any parameters

        Returns
        -------
        Doesn&#39;t return anything
        &#34;&#34;&#34;
        if self.isLoadedModel:
            print(&#34;[!!] You cannot look at epoch vs error graph in a loaded model&#34;)
            print(&#34;[!!] You can only look at that while training.!&#34;)
            print(&#34;[!!] Make some modifications to the network to own the model&#34;)
            return
        all_epochs = [i + 1 for i in range(self.epochs)]
        plt.xlabel(&#34;Epoch&#34;)
        plt.ylabel(&#34;Error&#34;)
        plt.title(&#34;Epoch vs Error&#34;)
        plt.plot(all_epochs, self.all_errors)
        plt.show()

    def evaluate(self):
        &#34;&#34;&#34;
        Print the basic information about the network.
        Like accuracy, error ..etc.

        Parameters
        ----------
        Doesn&#39;t accept any parameters

        Returns
        -------
        Doesn&#39;t return anything
        &#34;&#34;&#34;
        print(&#34;\t=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-&#34;)
        print(&#34;\tModel is trained for &#34;, self.epochs, &#34;epochs&#34;)
        print(&#34;\tModel Accuracy: &#34;, self.accuracy, &#34;%&#34;)
        if self.accuracy &lt; 70:
            print(&#34;\t\tModel Doesn&#39;t seem to have fit the data correctly&#34;)
            print(&#34;\t\tTry increasing the epochs&#34;)
        print(&#34;\t=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-&#34;)

    def display(self):
        &#34;&#34;&#34;
        Print the information of each layer of the network.
        It can be used to debug the network!

        Parameters
        ----------
        Doesn&#39;t accept any parameters

        Returns
        -------
        Doesn&#39;t return anything
        &#34;&#34;&#34;
        for i in range(self.total_layers):
            print(&#34;Layer: &#34;, i + 1)
            self.Network[i].display()

    def export_model(self, filename):
        &#34;&#34;&#34;
        Export the model to a json file

        Parameters
        ----------
        filename: str
            File name to export model

        Returns
        -------
        Doesn&#39;t return anything
        &#34;&#34;&#34;
        try:
            fhand = open(filename, &#34;w&#34;)
        except Exception as e:
            print(&#34;[!!] Unable to open file &#34;, filename, &#34;: &#34;, e)
            print(&#34;[!!] Couldn&#39;t export model&#34;)
            return

        model_info = dict()
        model_info[&#34;inputs&#34;] = self.I
        model_info[&#34;outputs&#34;] = self.O
        model_info[&#34;learning_rate&#34;] = self.learningRate
        model_info[&#34;model_compiled&#34;] = self.model_compiled
        model_info[&#34;layers&#34;] = list()
        for layer in self.Network:
            layer_object = dict()
            layer_object[&#34;neurons&#34;] = layer.num_nodes
            layer_object[&#34;inputs&#34;] = layer.inputs
            layer_object[&#34;weights&#34;] = layer.weights.tolist()
            layer_object[&#34;biases&#34;] = layer.biases.tolist()
            layer_object[&#34;activation_function&#34;] = layer.activation_function
            layer_object[&#34;loss_function&#34;] = layer.loss_function
            model_info[&#34;layers&#34;].append(layer_object)
        model_info[&#34;accuracy&#34;] = self.accuracy
        model_info[&#34;loss&#34;] = self.loss
        model_info[&#34;epochs&#34;] = self.epochs

        json_format_string = json.dumps(model_info)
        fhand.write(json_format_string)
        fhand.close()
        print(&#34;[*] Model exported successfully to&#34;, filename)

    @staticmethod
    def load_model(filename):
        &#34;&#34;&#34;
        Load model from an eported (json) model

        Parameters
        ----------
        filename : str
            Exported model (json) file

        Returns
        -------
        brain : NeuralNetwork
            NeuralNetwork object
        &#34;&#34;&#34;
        try:
            fhand = open(filename, &#34;r&#34;)
        except Exception as e:
            print(&#34;[!!] Unable to open file &#34;, filename, &#34;: &#34;, e)
            print(&#34;[!!] Couldn&#39;t load model&#34;)
            return

        model_info = json.load(fhand)
        # print(model_info)

        inputs = model_info[&#34;inputs&#34;]
        outputs = model_info[&#34;outputs&#34;]

        brain = NeuralNetwork(inputs, outputs)
        brain.total_layers = 0
        brain.isLoadedModel = True

        for layer_object in model_info[&#34;layers&#34;]:
            num_nodes = layer_object[&#34;neurons&#34;]
            weights = layer_object[&#34;weights&#34;]
            inputs = layer_object[&#34;inputs&#34;]
            biases = layer_object[&#34;biases&#34;]
            activation_fn = layer_object[&#34;activation_function&#34;]
            loss_function = layer_object[&#34;loss_function&#34;]

            layer = Layer(
                num_nodes, inputs, activation_function=activation_fn, loss_function=loss_function)
            layer.weights = np.array(weights)
            layer.biases = np.array(biases)
            brain.Network.append(layer)
            brain.total_layers += 1

        brain.accuracy = model_info[&#34;accuracy&#34;]
        brain.loss = model_info[&#34;loss&#34;]
        brain.epochs = model_info[&#34;epochs&#34;]
        brain.learningRate = model_info[&#34;learning_rate&#34;]
        brain.model_compiled = model_info[&#34;model_compiled&#34;]
        print(&#34;[*] (&#34;, filename, &#34;) Model Loaded successfully&#34;, sep=&#34;&#34;)

        return brain</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="nicenet.NeuralNetwork.NeuralNetwork.load_model"><code class="name flex">
<span>def <span class="ident">load_model</span></span>(<span>filename)</span>
</code></dt>
<dd>
<div class="desc"><p>Load model from an eported (json) model</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code></dt>
<dd>Exported model (json) file</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>brain</code></strong> :&ensp;<code><a title="nicenet.NeuralNetwork.NeuralNetwork" href="#nicenet.NeuralNetwork.NeuralNetwork">NeuralNetwork</a></code></dt>
<dd>NeuralNetwork object</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def load_model(filename):
    &#34;&#34;&#34;
    Load model from an eported (json) model

    Parameters
    ----------
    filename : str
        Exported model (json) file

    Returns
    -------
    brain : NeuralNetwork
        NeuralNetwork object
    &#34;&#34;&#34;
    try:
        fhand = open(filename, &#34;r&#34;)
    except Exception as e:
        print(&#34;[!!] Unable to open file &#34;, filename, &#34;: &#34;, e)
        print(&#34;[!!] Couldn&#39;t load model&#34;)
        return

    model_info = json.load(fhand)
    # print(model_info)

    inputs = model_info[&#34;inputs&#34;]
    outputs = model_info[&#34;outputs&#34;]

    brain = NeuralNetwork(inputs, outputs)
    brain.total_layers = 0
    brain.isLoadedModel = True

    for layer_object in model_info[&#34;layers&#34;]:
        num_nodes = layer_object[&#34;neurons&#34;]
        weights = layer_object[&#34;weights&#34;]
        inputs = layer_object[&#34;inputs&#34;]
        biases = layer_object[&#34;biases&#34;]
        activation_fn = layer_object[&#34;activation_function&#34;]
        loss_function = layer_object[&#34;loss_function&#34;]

        layer = Layer(
            num_nodes, inputs, activation_function=activation_fn, loss_function=loss_function)
        layer.weights = np.array(weights)
        layer.biases = np.array(biases)
        brain.Network.append(layer)
        brain.total_layers += 1

    brain.accuracy = model_info[&#34;accuracy&#34;]
    brain.loss = model_info[&#34;loss&#34;]
    brain.epochs = model_info[&#34;epochs&#34;]
    brain.learningRate = model_info[&#34;learning_rate&#34;]
    brain.model_compiled = model_info[&#34;model_compiled&#34;]
    print(&#34;[*] (&#34;, filename, &#34;) Model Loaded successfully&#34;, sep=&#34;&#34;)

    return brain</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="nicenet.NeuralNetwork.NeuralNetwork.Train"><code class="name flex">
<span>def <span class="ident">Train</span></span>(<span>self, dataset: List[Tuple[numpy.ndarray, numpy.ndarray]], size, epochs=100, logging=False, epoch_logging=True, prediction_evaulator=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Trains the neural network using the given dataset.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dataset</code></strong> :&ensp;<code>T_Dataset</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code></dt>
<dd>Size of the dataset</dd>
</dl>
<p>[epochs] : int
An optional parameter.
Number of epochs to train the network. Default value is 5000</p>
<p>[logging] : bool
An optional parameter.
If its true, all outputs from the network will be logged out onto STDOUT for each epoch.</p>
<p>[epoch_logging] : bool
An optional parameter.
If it is true, Error in each epoch will be logged to STDOUT.</p>
<p>[prediction_evaulator]: (prediction: T_Output_Array, target: T_Output_Array) -&gt; bool
An optional parameter.
Used to evaluate the fed forward output with the actual target.
Default value is 'Utils.judge_prediction' function.</p>
<h2 id="returns">Returns</h2>
<p>Doesn't return anything.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Train(self, dataset: T_Dataset, size, epochs=100, logging=False, epoch_logging=True, prediction_evaulator=None):
    &#34;&#34;&#34;
    Trains the neural network using the given dataset.

    Parameters
    ----------
    dataset : T_Dataset

    size : int
        Size of the dataset

    [epochs] : int
        An optional parameter.
        Number of epochs to train the network. Default value is 5000

    [logging] : bool
        An optional parameter.
        If its true, all outputs from the network will be logged out onto STDOUT for each epoch.

    [epoch_logging] : bool
        An optional parameter.
        If it is true, Error in each epoch will be logged to STDOUT.

    [prediction_evaulator]: (prediction: T_Output_Array, target: T_Output_Array) -&gt; bool
        An optional parameter.
        Used to evaluate the fed forward output with the actual target.
        Default value is &#39;Utils.judge_prediction&#39; function.

    Returns
    -------
    Doesn&#39;t return anything.
    &#34;&#34;&#34;

    if prediction_evaulator is not None:
        self.prediction_evaulator = prediction_evaulator

    if not self.model_compiled:
        print(&#34;[-] Network is not complete.!&#34;)
        print(&#34;[!!] Please compile the network by adding an output layer&#34;)
        return

    self.all_errors = list()
    self.epochs = epochs

    # Show progress bar if epoch_logging is False
    range_object = range(epochs)
    if not epoch_logging:
        range_object = tqdm(range(epochs), desc=&#34;Training...&#34;)

    for epoch in range_object:
        self.loss = 0
        # One Epoch
        if epoch == 0:
            start = time.time()
        elif epoch == 1:
            end = time.time()
            estimated_time = Utils.get_time_required(start, end, epochs)
            print(estimated_time)

        correct = 0
        for i in range(size):
            data_sample = dataset[i]
            # input_array = data_sample[0]
            # target_array = data_sample[1]
            input_array = data_sample[0]
            target_array = data_sample[1]

            all_outputs = self.feedforward(input_array)
            output_error, is_correct_output = self.backpropagate(
                target_array)
            self.update_weights(input_array)

            if is_correct_output:
                correct += 1
            self.loss += output_error
            if logging:
                print(
                    input_array.transpose(),
                    &#34;\x1b[35m&#34;,
                    all_outputs,
                    &#34;\x1b[0m&#34;,
                    target_array.transpose(),
                    &#34;\x1b[31m&#34;,
                    output_error,
                    &#34;\x1b[0m&#34;,
                )

        self.loss /= size
        self.all_errors.append(self.loss)
        self.accuracy = (correct*100)/size

        if epoch_logging:
            print(
                f&#34;Epoch: {epoch + 1} ==&gt; ({self.cost}) Error: {self.loss}, (%) Accuracy: {self.accuracy}&#34;)

        if logging:
            print()</code></pre>
</details>
</dd>
<dt id="nicenet.NeuralNetwork.NeuralNetwork.addLayer"><code class="name flex">
<span>def <span class="ident">addLayer</span></span>(<span>self, num_nodes, activation_function='sigmoid')</span>
</code></dt>
<dd>
<div class="desc"><p>Adds a layer to the network.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>num_nodes</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of nodes in the hidden layer</dd>
</dl>
<p>[activation_function] :str
It is an optional parameter.
Specifies the activation function of the layer.
Default value is sigmoid.</p>
<pre><code>Available options:
    sigmoid,
    tanh,
    linear,
    identity,
    softmax
</code></pre>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Doesn't return anything</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def addLayer(self, num_nodes, activation_function=&#34;sigmoid&#34;):
    &#34;&#34;&#34;
    Adds a layer to the network.

    Parameters
    ----------
    num_nodes : int
        Number of nodes in the hidden layer

    [activation_function] :str
        It is an optional parameter.
        Specifies the activation function of the layer.
        Default value is sigmoid.

        Available options:
            sigmoid,
            tanh,
            linear,
            identity,
            softmax

    Returns
    -------
    Doesn&#39;t return anything
    &#34;&#34;&#34;
    # A layer can be thought of as a matrix
    # No. of row = no. of nodes
    # No. of columns = No. of weights = No. of inputs + 1 (bias)
    if self.total_layers == 0:
        inputs = self.I
    else:
        last_layer = self.Network[-1]
        inputs = last_layer.num_nodes
    layer = Layer(num_nodes, inputs, activation_function, self.cost)
    self.Network.append(layer)
    self.total_layers += 1</code></pre>
</details>
</dd>
<dt id="nicenet.NeuralNetwork.NeuralNetwork.backpropagate"><code class="name flex">
<span>def <span class="ident">backpropagate</span></span>(<span>self, target: numpy.ndarray)</span>
</code></dt>
<dd>
<div class="desc"><p>Backpropagate the error throughout the network
This function is called inside the model only.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>target</code></strong> :&ensp;<code>np.array()</code></dt>
<dd>It is the ground truth value corresponding to the input.
It is columnar vector of size Outputs x 1</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>Error</code></strong> :&ensp;<code>float</code></dt>
<dd>
<h1 id="returns-the-mean-squared-error-of-the-particular-output">Returns the Mean Squared Error of the particular output</h1>
Returns the error using the specified loss function.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def backpropagate(self, target: T_Target_Array):
    &#34;&#34;&#34;
    Backpropagate the error throughout the network
    This function is called inside the model only.

    Parameters
    ----------
    target : np.array()
        It is the ground truth value corresponding to the input.
        It is columnar vector of size Outputs x 1

    Returns
    -------
    Error : float
        # Returns the Mean Squared Error of the particular output
        Returns the error using the specified loss function.
    &#34;&#34;&#34;
    for i in range(self.total_layers - 1, -1, -1):
        layer = self.Network[i]
        if i == self.total_layers - 1:
            # print(&#34;Output layer: &#34;, layer.output_array, &#34;Target: &#34;, target)
            output_errors = self.loss_computer.get_loss(
                layer.output_array, target)

            # Evaluate this prediction to compute accuracy
            is_correct_output: bool = self.prediction_evaulator(
                layer.output_array, target)

            # print(&#34;Error: &#34;, output_errors)
            layer.calculate_gradients(target, &#34;output&#34;)
        else:
            next_layer = self.Network[i + 1]
            layer.calculate_gradients(
                next_layer.weights, &#34;hidden&#34;, next_layer.deltas
            )
    return output_errors, is_correct_output</code></pre>
</details>
</dd>
<dt id="nicenet.NeuralNetwork.NeuralNetwork.compile"><code class="name flex">
<span>def <span class="ident">compile</span></span>(<span>self, activation_function='sigmoid')</span>
</code></dt>
<dd>
<div class="desc"><p>Basically, it just adds the output layer to the network.</p>
<h2 id="parameters">Parameters</h2>
<p>[activation_function] :str
It is an optional parameter.
Specifies the activation function of the layer.
Default value is sigmoid.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Doesn't return anything</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compile(self, activation_function=&#34;sigmoid&#34;):
    &#34;&#34;&#34;
    Basically, it just adds the output layer to the network.

    Parameters
    ----------
    [activation_function] :str
        It is an optional parameter.
        Specifies the activation function of the layer.
        Default value is sigmoid.

    Returns
    -------
    Doesn&#39;t return anything
    &#34;&#34;&#34;
    if self.model_compiled:
        print(&#34;[!!] Model is already compiled!&#34;)
        print(&#34;[!!] You cannot add layers anymore&#34;)
        return
    self.isLoadedModel = False
    self.model_compiled = True
    # Adding output layer
    self.addLayer(self.O, activation_function=activation_function)</code></pre>
</details>
</dd>
<dt id="nicenet.NeuralNetwork.NeuralNetwork.display"><code class="name flex">
<span>def <span class="ident">display</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Print the information of each layer of the network.
It can be used to debug the network!</p>
<h2 id="parameters">Parameters</h2>
<p>Doesn't accept any parameters</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Doesn't return anything</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def display(self):
    &#34;&#34;&#34;
    Print the information of each layer of the network.
    It can be used to debug the network!

    Parameters
    ----------
    Doesn&#39;t accept any parameters

    Returns
    -------
    Doesn&#39;t return anything
    &#34;&#34;&#34;
    for i in range(self.total_layers):
        print(&#34;Layer: &#34;, i + 1)
        self.Network[i].display()</code></pre>
</details>
</dd>
<dt id="nicenet.NeuralNetwork.NeuralNetwork.epoch_vs_error"><code class="name flex">
<span>def <span class="ident">epoch_vs_error</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Plot error vs epoch graph</p>
<h2 id="parameters">Parameters</h2>
<p>Doesn't accept any parameters</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Doesn't return anything</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def epoch_vs_error(self):
    &#34;&#34;&#34;
    Plot error vs epoch graph

    Parameters
    ----------
    Doesn&#39;t accept any parameters

    Returns
    -------
    Doesn&#39;t return anything
    &#34;&#34;&#34;
    if self.isLoadedModel:
        print(&#34;[!!] You cannot look at epoch vs error graph in a loaded model&#34;)
        print(&#34;[!!] You can only look at that while training.!&#34;)
        print(&#34;[!!] Make some modifications to the network to own the model&#34;)
        return
    all_epochs = [i + 1 for i in range(self.epochs)]
    plt.xlabel(&#34;Epoch&#34;)
    plt.ylabel(&#34;Error&#34;)
    plt.title(&#34;Epoch vs Error&#34;)
    plt.plot(all_epochs, self.all_errors)
    plt.show()</code></pre>
</details>
</dd>
<dt id="nicenet.NeuralNetwork.NeuralNetwork.evaluate"><code class="name flex">
<span>def <span class="ident">evaluate</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Print the basic information about the network.
Like accuracy, error ..etc.</p>
<h2 id="parameters">Parameters</h2>
<p>Doesn't accept any parameters</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Doesn't return anything</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def evaluate(self):
    &#34;&#34;&#34;
    Print the basic information about the network.
    Like accuracy, error ..etc.

    Parameters
    ----------
    Doesn&#39;t accept any parameters

    Returns
    -------
    Doesn&#39;t return anything
    &#34;&#34;&#34;
    print(&#34;\t=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-&#34;)
    print(&#34;\tModel is trained for &#34;, self.epochs, &#34;epochs&#34;)
    print(&#34;\tModel Accuracy: &#34;, self.accuracy, &#34;%&#34;)
    if self.accuracy &lt; 70:
        print(&#34;\t\tModel Doesn&#39;t seem to have fit the data correctly&#34;)
        print(&#34;\t\tTry increasing the epochs&#34;)
    print(&#34;\t=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-&#34;)</code></pre>
</details>
</dd>
<dt id="nicenet.NeuralNetwork.NeuralNetwork.export_model"><code class="name flex">
<span>def <span class="ident">export_model</span></span>(<span>self, filename)</span>
</code></dt>
<dd>
<div class="desc"><p>Export the model to a json file</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code></dt>
<dd>File name to export model</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Doesn't return anything</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def export_model(self, filename):
    &#34;&#34;&#34;
    Export the model to a json file

    Parameters
    ----------
    filename: str
        File name to export model

    Returns
    -------
    Doesn&#39;t return anything
    &#34;&#34;&#34;
    try:
        fhand = open(filename, &#34;w&#34;)
    except Exception as e:
        print(&#34;[!!] Unable to open file &#34;, filename, &#34;: &#34;, e)
        print(&#34;[!!] Couldn&#39;t export model&#34;)
        return

    model_info = dict()
    model_info[&#34;inputs&#34;] = self.I
    model_info[&#34;outputs&#34;] = self.O
    model_info[&#34;learning_rate&#34;] = self.learningRate
    model_info[&#34;model_compiled&#34;] = self.model_compiled
    model_info[&#34;layers&#34;] = list()
    for layer in self.Network:
        layer_object = dict()
        layer_object[&#34;neurons&#34;] = layer.num_nodes
        layer_object[&#34;inputs&#34;] = layer.inputs
        layer_object[&#34;weights&#34;] = layer.weights.tolist()
        layer_object[&#34;biases&#34;] = layer.biases.tolist()
        layer_object[&#34;activation_function&#34;] = layer.activation_function
        layer_object[&#34;loss_function&#34;] = layer.loss_function
        model_info[&#34;layers&#34;].append(layer_object)
    model_info[&#34;accuracy&#34;] = self.accuracy
    model_info[&#34;loss&#34;] = self.loss
    model_info[&#34;epochs&#34;] = self.epochs

    json_format_string = json.dumps(model_info)
    fhand.write(json_format_string)
    fhand.close()
    print(&#34;[*] Model exported successfully to&#34;, filename)</code></pre>
</details>
</dd>
<dt id="nicenet.NeuralNetwork.NeuralNetwork.feedforward"><code class="name flex">
<span>def <span class="ident">feedforward</span></span>(<span>self, input_array: numpy.ndarray)</span>
</code></dt>
<dd>
<div class="desc"><p>Feeds the given input throughout the network</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>input_array</code></strong> :&ensp;<code>T_Feature_Array</code></dt>
<dd>Input to be fed to the network.
It is columnar vector of size Inputs x 1</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>all_outputs</code></strong> :&ensp;<code>T_Output_Array</code></dt>
<dd>An array of all the outputs produced by each layer.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def feedforward(self, input_array: T_Feature_Array):
    &#34;&#34;&#34;
    Feeds the given input throughout the network

    Parameters
    ----------
    input_array : T_Feature_Array
        Input to be fed to the network.
        It is columnar vector of size Inputs x 1

    Returns
    -------
    all_outputs : T_Output_Array
        An array of all the outputs produced by each layer.
    &#34;&#34;&#34;
    all_outputs: typing.List[T_Output_Array] = list()
    _i = 1
    for layer in self.Network:
        # print(&#34;Feeding &#34;, input_array.T, &#34;to , layer&#34;, i)
        outputs = layer.feed(input_array)
        all_outputs.append(outputs.T)
        input_array = outputs
        # print(&#34;All outputs: &#34;, all_outputs)
        # print()
        # i += 1
    return all_outputs</code></pre>
</details>
</dd>
<dt id="nicenet.NeuralNetwork.NeuralNetwork.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, input_array: numpy.ndarray)</span>
</code></dt>
<dd>
<div class="desc"><p>Predicts the output using a given input.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>input_array</code></strong> :&ensp;<code>np.array()</code></dt>
<dd>It is columnar vector of size Inputs x 1
It is the input fed to the network</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>prediction</code></strong> :&ensp;<code>np.array()</code></dt>
<dd>Predicted value produced by the network.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, input_array: T_Feature_Array):
    &#34;&#34;&#34;
    Predicts the output using a given input.

    Parameters
    ----------
    input_array : np.array()
        It is columnar vector of size Inputs x 1
        It is the input fed to the network

    Returns
    -------
    prediction : np.array()
        Predicted value produced by the network.
    &#34;&#34;&#34;
    return self.feedforward(input_array)[-1]</code></pre>
</details>
</dd>
<dt id="nicenet.NeuralNetwork.NeuralNetwork.setLearningRate"><code class="name flex">
<span>def <span class="ident">setLearningRate</span></span>(<span>self, lr)</span>
</code></dt>
<dd>
<div class="desc"><p>Modifies the learning rate of the network.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>lr</code></strong> :&ensp;<code>float</code></dt>
<dd>New learning rate</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Doesn't return anything</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def setLearningRate(self, lr):
    &#34;&#34;&#34;
    Modifies the learning rate of the network.

    Parameters
    ----------
    lr : float
        New learning rate

    Returns
    -------
    Doesn&#39;t return anything
    &#34;&#34;&#34;
    self.learningRate = lr</code></pre>
</details>
</dd>
<dt id="nicenet.NeuralNetwork.NeuralNetwork.update_weights"><code class="name flex">
<span>def <span class="ident">update_weights</span></span>(<span>self, input_array: numpy.ndarray)</span>
</code></dt>
<dd>
<div class="desc"><p>Update the weights of the network.
This function is called inside the model only.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>input_array</code></strong> :&ensp;<code>np.array()</code></dt>
<dd>It is the input fed to the network
It is columnar vector of size Inputs x 1</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Doesn't return anything</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_weights(self, input_array: T_Feature_Array):
    &#34;&#34;&#34;
    Update the weights of the network.
    This function is called inside the model only.

    Parameters
    ----------
    input_array : np.array()
        It is the input fed to the network
        It is columnar vector of size Inputs x 1

    Returns
    -------
    Doesn&#39;t return anything
    &#34;&#34;&#34;
    for i in range(self.total_layers - 1, -1, -1):
        layer = self.Network[i]
        if i == 0:
            # if it is the first layer =&gt; inputs = input_array
            layer.update_weights(input_array, self.learningRate)
        else:
            # not the first most =&gt; inputs = previous layer&#39;s output
            inputs = self.Network[i - 1].output_array
            layer.update_weights(inputs, self.learningRate)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="nicenet" href="index.html">nicenet</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="nicenet.NeuralNetwork.NeuralNetwork" href="#nicenet.NeuralNetwork.NeuralNetwork">NeuralNetwork</a></code></h4>
<ul class="two-column">
<li><code><a title="nicenet.NeuralNetwork.NeuralNetwork.Train" href="#nicenet.NeuralNetwork.NeuralNetwork.Train">Train</a></code></li>
<li><code><a title="nicenet.NeuralNetwork.NeuralNetwork.addLayer" href="#nicenet.NeuralNetwork.NeuralNetwork.addLayer">addLayer</a></code></li>
<li><code><a title="nicenet.NeuralNetwork.NeuralNetwork.backpropagate" href="#nicenet.NeuralNetwork.NeuralNetwork.backpropagate">backpropagate</a></code></li>
<li><code><a title="nicenet.NeuralNetwork.NeuralNetwork.compile" href="#nicenet.NeuralNetwork.NeuralNetwork.compile">compile</a></code></li>
<li><code><a title="nicenet.NeuralNetwork.NeuralNetwork.display" href="#nicenet.NeuralNetwork.NeuralNetwork.display">display</a></code></li>
<li><code><a title="nicenet.NeuralNetwork.NeuralNetwork.epoch_vs_error" href="#nicenet.NeuralNetwork.NeuralNetwork.epoch_vs_error">epoch_vs_error</a></code></li>
<li><code><a title="nicenet.NeuralNetwork.NeuralNetwork.evaluate" href="#nicenet.NeuralNetwork.NeuralNetwork.evaluate">evaluate</a></code></li>
<li><code><a title="nicenet.NeuralNetwork.NeuralNetwork.export_model" href="#nicenet.NeuralNetwork.NeuralNetwork.export_model">export_model</a></code></li>
<li><code><a title="nicenet.NeuralNetwork.NeuralNetwork.feedforward" href="#nicenet.NeuralNetwork.NeuralNetwork.feedforward">feedforward</a></code></li>
<li><code><a title="nicenet.NeuralNetwork.NeuralNetwork.load_model" href="#nicenet.NeuralNetwork.NeuralNetwork.load_model">load_model</a></code></li>
<li><code><a title="nicenet.NeuralNetwork.NeuralNetwork.predict" href="#nicenet.NeuralNetwork.NeuralNetwork.predict">predict</a></code></li>
<li><code><a title="nicenet.NeuralNetwork.NeuralNetwork.setLearningRate" href="#nicenet.NeuralNetwork.NeuralNetwork.setLearningRate">setLearningRate</a></code></li>
<li><code><a title="nicenet.NeuralNetwork.NeuralNetwork.update_weights" href="#nicenet.NeuralNetwork.NeuralNetwork.update_weights">update_weights</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>